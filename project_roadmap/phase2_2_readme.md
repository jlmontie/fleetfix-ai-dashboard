# Phase 2.2: Text-to-SQL Converter

## What This Does

Converts natural language queries to SQL using LLM (Claude or GPT-4), validates for safety, and executes against the database.

**Complete pipeline:**
```
User: "Show me vehicles overdue for maintenance"
  â†“
Schema Context Builder (provides DB schema to LLM)
  â†“
LLM (generates SQL)
  â†“
SQL Validator (checks safety)
  â†“
Database (executes query)
  â†“
Results returned to user
```

## Files Created

```
backend/ai_agent/
â”œâ”€â”€ text_to_sql.py              # LLM integration
â”œâ”€â”€ sql_validator.py            # Safety validation
â””â”€â”€ test_text_to_sql.py         # Integration tests
```

## Prerequisites

1. **Phase 2.1 complete** (Schema Context Builder working)
2. **LLM API key** (either Anthropic or OpenAI)

## Setup

### 1. Install Dependencies

Add to `requirements.txt`:
```
anthropic==0.18.1
openai==1.10.0
```

Install:
```bash
cd backend
source venv/bin/activate
pip install anthropic openai
```

### 2. Add API Key

Edit `.env`:
```bash
# Choose one:
ANTHROPIC_API_KEY=your_anthropic_key_here
# OR
OPENAI_API_KEY=your_openai_key_here
```

**Get API keys:**
- Anthropic: https://console.anthropic.com/
- OpenAI: https://platform.openai.com/api-keys

### 3. Copy Files

Place artifacts in:
- `backend/ai_agent/text_to_sql.py`
- `backend/ai_agent/sql_validator.py`
- `backend/ai_agent/test_text_to_sql.py`

### 4. Run Tests

```bash
python ai_agent/test_text_to_sql.py
```

## Expected Output

```
======================================================================
FLEETFIX TEXT-TO-SQL SYSTEM
Complete Integration Test Suite
======================================================================

1. Building schema context...
âœ“ Schema context built (8000 chars, ~2000 tokens)

2. Initializing LLM converter...
âœ“ Using anthropic LLM

3. Initializing SQL validator...
âœ“ SQL validator ready

======================================================================
Running Test Queries
======================================================================

Test 1: Time-based filtering with CURRENT_DATE
Query: "Show me vehicles that are overdue for maintenance"
----------------------------------------------------------------------

Generating SQL with LLM...
âœ“ SQL generated (confidence: 0.95)

Generated SQL:
SELECT id, make, model, license_plate, next_service_due,
       CURRENT_DATE - next_service_due as days_overdue
FROM vehicles
WHERE next_service_due < CURRENT_DATE
ORDER BY days_overdue DESC;

Validating SQL...
âœ“ SQL validation passed

Checking for expected keywords...
âœ“ All expected keywords present

Executing SQL against database...
âœ“ Query executed successfully
  Rows returned: 3
  Columns: id, make, model, license_plate, next_service_due, days_overdue

  Sample results (first 3 rows):
    Row 1: {'id': 5, 'make': 'Ford', 'model': 'Transit', ...}
    Row 2: {'id': 12, 'make': 'Ram', 'model': 'ProMaster', ...}
    Row 3: {'id': 18, 'make': 'Chevrolet', 'model': 'Silverado', ...}

Explanation:
  This query finds vehicles where the next_service_due date has passed...

âœ“ Test 1 PASSED

[... Tests 2-5 ...]

======================================================================
Test Summary
======================================================================
Total tests: 5
Passed: 5
Failed: 0
Success rate: 100.0%

âœ“ All integration tests passed!

======================================================================
Security Test: Malicious Query Prevention
======================================================================

Testing malicious query blocking...
âœ“ BLOCKED: DELETE FROM vehicles WHERE id = 1...
âœ“ BLOCKED: DROP TABLE drivers...
âœ“ BLOCKED: UPDATE vehicles SET status = 'inactive'...
âœ“ BLOCKED: INSERT INTO vehicles VALUES (1, 'test')...
âœ“ BLOCKED: SELECT * FROM vehicles; DELETE FROM drivers;...
âœ“ BLOCKED: SELECT * FROM vehicles WHERE 1=1 OR '1'='1'...

âœ“ All malicious queries blocked successfully

======================================================================
Conversation Context Test
======================================================================

User: Show me vehicles with critical fault codes
SQL: SELECT v.* FROM vehicles v JOIN fault_codes fc...

User: Now show me their maintenance history (follow-up)
SQL: SELECT mr.* FROM maintenance_records mr JOIN vehicles v...

âœ“ Conversation context working

======================================================================
FINAL TEST SUMMARY
======================================================================
âœ“ PASS: Integration Test
âœ“ PASS: Security Test
âœ“ PASS: Conversation Context

Overall: 3/3 test suites passed

======================================================================
ðŸŽ‰ SUCCESS! Text-to-SQL system is fully functional!
======================================================================
```

## How It Works

### Text-to-SQL Converter

**Basic Usage:**
```python
from ai_agent.schema_context import SchemaContextBuilder
from ai_agent.text_to_sql import TextToSQLConverter

# Build schema context
builder = SchemaContextBuilder()
schema_context = builder.build_schema_context()

# Initialize converter
converter = TextToSQLConverter(
    schema_context=schema_context,
    provider="anthropic"  # or "openai"
)

# Convert query
result = converter.convert("Show me vehicles overdue for maintenance")

print(result.sql)           # Generated SQL
print(result.explanation)   # Why this SQL was chosen
print(result.confidence)    # 0.0-1.0
print(result.warnings)      # Any caveats
```

### SQL Validator

**Usage:**
```python
from ai_agent.sql_validator import SQLValidator

validator = SQLValidator()

# Validate SQL
validation = validator.validate(sql_query)

if validation.is_valid:
    # Safe to execute
    execute_query(validation.sanitized_sql)
else:
    # Show errors
    print(validation.errors)
```

### Conversation Context

**For follow-up queries:**
```python
# First query
result1 = converter.convert("Show critical fault codes")

# Follow-up with context
history = [("Show critical fault codes", result1.sql)]
result2 = converter.convert_with_conversation_history(
    "Show their maintenance history",
    conversation_history=history
)
```

## Safety Features

### SQL Validator Blocks:

- âœ“ DELETE, UPDATE, INSERT, DROP
- âœ“ Multiple statements (SQL injection)
- âœ“ SQL comments (-- and /* */)
- âœ“ UNION-based injection
- âœ“ Classic injection patterns (OR 1=1)
- âœ“ System commands (xp_cmdshell, EXEC)

### LLM Prompt Engineering:

- Clear instructions: "Only generate SELECT queries"
- Schema context: Full table/column descriptions
- Example patterns: Shows proper CURRENT_DATE usage
- Explicit rules: 10 specific guidelines

## Token Usage

**Per query:**
- Schema context: ~2000 tokens
- User query: ~20 tokens
- LLM response: ~300 tokens
- **Total: ~2320 tokens per query**

**Cost (approximate):**
- Claude Sonnet: $0.003 per query ($0.03 per 10 queries)
- GPT-4: $0.02 per query ($0.20 per 10 queries)

**Recommendation:** Use Claude Sonnet for cost efficiency

## Supported Query Types

### âœ“ Working Well:

1. **Simple filtering**
   - "Show active vehicles"
   - "List overdue maintenance"

2. **Time-based queries**
   - "Yesterday's performance"
   - "Last 7 days"
   - "This month"

3. **Aggregations**
   - "Average fuel efficiency"
   - "Top 5 drivers"
   - "Count by vehicle type"

4. **Joins**
   - "Drivers with poor scores"
   - "Vehicles with fault codes"
   - "Maintenance costs by vehicle"

5. **Complex combinations**
   - "Show vehicles overdue with critical faults"
   - "Driver performance trends by vehicle type"

### âš  May Need Refinement:

1. **Ambiguous queries**
   - "Show me problems" (what kind?)
   - "Find issues" (too vague)

2. **Domain-specific jargon**
   - Might need prompt tuning

3. **Complex business logic**
   - Multi-step calculations
   - Custom definitions

## Troubleshooting

### "No API key found"

**Problem:** LLM API key not set

**Fix:**
```bash
# Check .env file
cat .env | grep API_KEY

# Add key if missing
echo "ANTHROPIC_API_KEY=sk-ant-..." >> .env
```

### "SQL validation failed"

**Problem:** LLM generated dangerous SQL

**Cause:** Usually prompt issues or malicious user input

**Fix:** Check the generated SQL and validation errors. The validator is working correctly by blocking it.

### "Import error: anthropic/openai"

**Problem:** Package not installed

**Fix:**
```bash
pip install anthropic openai
```

### "Query returns no results"

**Problem:** SQL is valid but no matching data

**Not an error:** This is expected for some queries based on your data

### LLM generates wrong SQL

**Possible causes:**
1. Schema context missing information
2. Query is ambiguous
3. Prompt needs tuning

**Debug:**
1. Check `schema_context_full.txt` - is it clear?
2. Try more specific query
3. Review LLM explanation for reasoning

## Customization

### Change LLM Model

```python
# Use different Claude model
converter = TextToSQLConverter(
    schema_context,
    provider="anthropic",
    model="claude-opus-4-20250514"  # More capable, more expensive
)

# Use different GPT model
converter = TextToSQLConverter(
    schema_context,
    provider="openai",
    model="gpt-4-turbo"
)
```

### Adjust Prompt

Edit `text_to_sql.py`, method `_build_prompt()`:
```python
def _build_prompt(self, user_query: str) -> str:
    # Customize prompt here
    # Add more examples
    # Change instructions
    # Add business rules
```

### Add Custom Validation Rules

Edit `sql_validator.py`:
```python
class SQLValidator:
    # Add to FORBIDDEN_KEYWORDS
    FORBIDDEN_KEYWORDS = {
        'DELETE', 'DROP', ...
        'YOUR_KEYWORD'  # Add custom
    }
    
    # Add to SUSPICIOUS_PATTERNS
    SUSPICIOUS_PATTERNS = [
        ...
        r'YOUR_PATTERN'  # Add custom regex
    ]
```

## Performance

- **Schema context build:** ~50ms
- **LLM API call:** 1-3 seconds
- **SQL validation:** <10ms
- **Query execution:** 50ms-500ms (depends on query)
- **Total:** ~1.5-4 seconds end-to-end

## Next Steps

Once Phase 2.2 is working:

1. âœ“ Natural language â†’ SQL working
2. âœ“ Safety validation passing
3. âœ“ Database execution successful

**Next: Phase 2.3**
- Query executor with error handling
- Result formatting
- Insight generation

**Then: Phase 3**
- FastAPI endpoints
- Dashboard integration
- Daily digest

## Quick Test

Run this to verify everything works:

```bash
python ai_agent/test_text_to_sql.py
```

Should see: **"ðŸŽ‰ SUCCESS! Text-to-SQL system is fully functional!"**

---

**Status:** Phase 2.2 Complete  
**Ready for:** Phase 2.3 (Query Executor & Insights)